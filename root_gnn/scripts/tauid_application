#!/usr/bin/env python

import argparse
import root_gnn
from root_gnn import trainer
import os

import numpy as np

from graph_nets import utils_tf
import sonnet as snt

from root_gnn import model
from root_gnn.utils import load_yaml
from root_gnn import utils_plot

from root_gnn.trainer import read_dataset
from root_gnn.trainer import loop_dataset
from ROOT import TChain, AddressOf, std

from root_gnn.src.datasets.base import DataSet
from root_gnn import datasets as DataSets

import tensorflow as tf

if __name__ == "__main__":

    parser = argparse.ArgumentParser(description='Predict with GNN')
    add_arg = parser.add_argument
    add_arg("-i","--inputfile", help="Path to the input root file")
    add_arg("-o","--outputfile", help="Path of the output root file")
    add_arg("-n","--num-events", default=None,  help="Number of events for inference")
    add_arg("--initial-event", default=0, help="Event number to start at")
   # add_arg("--modeldir", help="Overwrite the model directory from the configuration file", default=None)
    add_arg("--config",help="configuration file name",metavar='N',type=str,nargs='+')
    add_arg('--type', default="TauIdentificationDataset", help='which data to process',
            choices=list(DataSets.__all__))
    add_arg('--type-arg', default="", help='argument to pass to the dataset object constructor if any')

    args = parser.parse_args()

    ntuple_in = args.inputfile
    ntuple_out = args.outputfile
    
    from root_gnn import datasets as DataSets # the main program to produce the graph
    from root_gnn.datasets import *

    data = eval(f"{args.type}({args.type_arg})")
    if args.config is not None:
        data.set_config_file(args.config[0])

    models = []
    iters = []
    for file in args.config:
        config = load_yaml(file)
        modeldir = os.path.join(config['output_dir'], 'checkpoints')
        model_name = config['model']
        mode = 'clf' if "Classifier" in model_name else 'rgr'
        mode += ',globals' if "Global" in model_name else ',edges'
        config['mode'] = mode

        if 'loss_pars' in config and config['loss_pars']:
            loss_fcn = config['loss_name']+','+config['loss_pars']
            config['loss_fcn'] = loss_fcn

        config['optimizer'] = config['learning_rate']
        print(config)
        trnr = trainer.Trainer(**config)
        trnr._setup_training_loop()
        trnr.make_checkpoints()
        models.append((trnr.model,config['num_iters']))



    # Apply the model to the jets in the root file
    from root_gnn.datasets import TauIdentificationDataset
    import ROOT
    from ROOT import TFile, TTree, TChain, AddressOf, std
    from array import array

    tauid = data
    tree_name = "output"
    chain = TChain(tree_name,tree_name)
    chain.Add(ntuple_in)
    n_entries = chain.GetEntries()

    # Create a new file and a new branch
    newfile = TFile(ntuple_out,"RECREATE")
    newtree = chain.CloneTree(0)
    leafValues = std.vector('float')()
    newbranch = newtree.Branch("TauPrediction",leafValues)
    if not args.num_events is None:
        end_entry = min(n_entries,initial_entry+int(args.num_events))
    initial_entry = min(n_entries,int(args.initial_event))

    # Iterate over the entries in the input file
    for ientry in range(initial_entry,end_entry):
        chain.GetEntry(ientry)
        graph_list = tauid.make_graph(chain)
        # Generate graphs and predictions
        leafValues.clear()
        for graph in graph_list:
            prediction = 0.
            if graph[0] == None:
                continue
            for model in models:
                outputs = model[0](graph[0], model[1],is_training=False)
                #extract gnn output
                prediction += np.float32(outputs[-1].globals[0][0])
            prediction /= len(models)
            leafValues.push_back(prediction)
        newtree.Fill()
    newtree.Write()

