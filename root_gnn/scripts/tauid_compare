#!/usr/bin/env python
import argparse
import os

import numpy as np

from ROOT import TChain, AddressOf, std

import matplotlib.pyplot as plt

import sklearn.metrics
from sklearn.metrics import mean_squared_error, auc, roc_curve, precision_recall_curve
import sys

parser = argparse.ArgumentParser(description='Generate plots for comparing the performance of tau identifers')
parser.add_argument('--npz', nargs='+', help='Files with predictions and truth. Must be from the same events to be compatible')
parser.add_argument('--qcd', nargs='+', default=[None], help='Files with predictions and truth from qcd events to perform inference on')
parser.add_argument('--other', nargs='+', default=[None], help='Files with predictions and truth from other-prong events to perform inference on')
parser.add_argument('--colors', nargs='+', default=None, help='Colors to plot with')
parser.add_argument('--styles', nargs='+', default=None, help='Line styles. e.g. \'solid\',\'dashed\', or \'dotted\'')
parser.add_argument('--legend', nargs='+', help='Labels to use in the plot legends')
parser.add_argument('--prongs', nargs='+', default=None, help='Labels to use in the plot legends')
parser.add_argument('--prefix', default=None, help='Prefix for the files')
parser.add_argument('--efficiencies','-e',default=[0.60,0.75,0.85],help='Find rejection power associated with interesting efficiencies and place them in a table')
parser.add_argument('--pad-zero', '-p', action='store_true', help='Pad zero fpr')
parser.add_argument('--no-log', action='store_true', help='Do not use Log Scale')
parser.add_argument('-y','--y', help='Quantity to use on y-axis for rejection plot', default='Rejection')
parser.add_argument('--log-x', help="Use log scale for x-axis", action='store_true')
parser.add_argument('--log-x-roc', help="Use log scale for x-axis for ROC", action='store_true')
parser.add_argument('--title', help='title for the plot', default=None)
parser.add_argument('--num-params', nargs='+', default=None, help='Number of trainable params')
args = parser.parse_args()

small = 10
medium = 12
large = 15

efficiencies_3prong = [0.45,0.60,0.75,0.95]
efficiencies_1prong = [0.60,0.75,0.85,0.95]
working_ponts = ["Tight","Medium","Loose","Very loose"]

plt.rc('font',size=medium)
plt.rc('figure',titlesize=large)
plt.rc('axes',labelsize=medium)

def buildROC(target_test,test_preds,color='r',style='solid',threshold=0.5):
    y_true = target_test > threshold
    fpr, tpr, _ = roc_curve(y_true, test_preds)
    plt.plot(fpr, tpr, color=color, linestyle=style, lw=2.0)
    return auc(fpr, tpr)

def buildRejection(target_test,test_preds,color='r',style='solid',efficiencies=[], pad_zero=True, y=False):
    fpr, tpr, threshold = roc_curve(target_test, test_preds)
    x = tpr
    if pad_zero:
        for i in range(len(fpr)):
            if fpr[i] == 0:
                fpr[i] = 1e-6
    if y == 'rej_ratio':
        y = (1.0/fpr-1) / len([i for i in target_test if i == 0])
    elif y == 'Rejection':
        y = 1.0/fpr
    elif y == 'fpr':
        y = fpr
    elif y == 'tn':
        y = (1-fpr) * len([i for i in target_test if i == 0])
    elif y == 'tnr':
        y = (1-fpr)
    elif y == 'pe':
        p, r, t = precision_recall_curve(target_test, test_preds)
        x = r
        y = p
    plt.plot(x, y, color=color, linestyle=style, lw=2.0)
    #Make table of rejection power for given efficiencies
    rejections = []
    for e in efficiencies:
        for idx in range(0,len(x)):
            if x[idx] > e:
                # keep track of 1/fpr for e
                rejections.append(y[idx])
                break
    return efficiencies,rejections

if __name__ == "__main__":
    n_plots = len(args.npz)
    target = []
    predictions = []
    if args.other[0] != None and len(args.npz) == len(args.other):
        for dataset, qcd, other in zip(args.npz,args.qcd, args.other):
            array1 = np.load(dataset)
            array2 = np.load(qcd)
            array3 = np.load(other)
            predictions.append(np.concatenate([array1['predictions'], array2['predictions'], array3['predictions']]))
            target.append(np.concatenate([array1['truth_info'], array2['truth_info'], array3['truth_info']]))
            
    elif args.qcd[0] != None and len(args.npz) == len(args.qcd):
        #handle optional use of qcd
        for dataset, qcd in zip(args.npz,args.qcd):
            array1 = np.load(dataset)
            #print(len(array1['predictions']))
            array2 = np.load(qcd)
            predictions.append(np.concatenate([array1['predictions'],array2['predictions']]))
            target.append(np.concatenate([array1['truth_info'],array2['truth_info']]))
            
    else:
        for dataset in args.npz:
            array = np.load(dataset)
            predictions.append(array['predictions'])
            target.append(array['truth_info'])
            
    if args.colors is None:
        args.colors = plt.rcParams['axes.prop_cycle'].by_key()['color']
    if args.styles is None:
        args.styles = ['solid'] * n_plots
    if args.prongs is None:
        args.prongs = ['inclusive'] * n_plots
    if args.num_params is None:
        args.num_params = ['N/A'] * n_plots
        
    print(">>> Start Plotting ROC!")
    plt.figure(figsize=(12, 8))
    plt.title('Receiver Operating Characteristic')
    if args.log_x_roc:
        plt.xscale('log')
    plt.ylabel('True Positive Rate')
    plt.xlabel('False Positive Rate')
    all_auc = []
    for idx in range(0,n_plots):
        roc_auc=buildROC(target[idx],predictions[idx],args.colors[idx],args.styles[idx])
        #args.legend[idx] += f" (AUC {roc_auc:.4f})"
        all_auc.append(roc_auc)
    
    plt.legend(args.legend,loc='lower right')
    plt.savefig(args.prefix+'_roc.pdf')
    plt.close()
    
    print(">>> Start Plotting Rejection Curve!")
    plt.figure(figsize=(12, 8))
    if not args.no_log:
        plt.yscale('log')
    if args.log_x:
        plt.xscale('log')
    plt.ylabel(r'Fake $ \tau_{\mathrm{had-vis}}$ ' + str(args.y))
    plt.xlabel(r'True $ \tau_{\mathrm{had-vis}}$ Efficiency')
    plt.grid()
    rejections = []
    efficiencies = []
    with open(args.prefix+'_table.csv',"w") as t:
        if all([i != '1' for i in args.prongs]):
            eff = efficiencies_3prong
            column_string = f"Model/{args.y},AUC,Number of Trainable Params,{working_ponts[0]} ({eff[0]}),{working_ponts[1]} ({eff[1]}),{working_ponts[2]} ({eff[2]}),{working_ponts[3]} ({eff[3]})\n"
        else:
            column_string = f"Model/{args.y},AUC,Number of Trainable Params,{working_ponts[0]},{working_ponts[1]},{working_ponts[2]},{working_ponts[3]}\n"
        t.write(column_string)
        for idx in range(0,n_plots):
            table_str = f"{args.legend[idx]},{all_auc[idx]:.4f},{args.num_params[idx]},"

            efficiency,rejection=buildRejection(target[idx],
                                                predictions[idx],
                                                args.colors[idx],
                                                args.styles[idx],
                                                efficiencies=(efficiencies_1prong if args.prongs[idx] == '1' else efficiencies_3prong),
                                                pad_zero=args.pad_zero,
                                                y=args.y)
            for r in rejection:
                table_str += f"{r:.1f},"

            efficiencies.append(efficiency)
            rejections.append(rejection)
            t.write(table_str+"\n")
        for idx in range(0,n_plots):
            plt.plot(efficiencies[idx],rejections[idx],color=args.colors[idx],marker='o',linestyle='')
    if not args.title is None:
        plt.title(args.title)
    plt.legend(args.legend,loc='lower left')
    plt.savefig(args.prefix+'_rejection.pdf')
    plt.close()
