#!/usr/bin/env python
import time
import os
from tkinter.messagebox import NO

from root_gnn import datasets as DataSets

if __name__ == "__main__":
    import argparse
    parser = argparse.ArgumentParser(description='Create TFRecord of graphs for training')
    add_arg = parser.add_argument
    add_arg("input_file", help='input file')
    add_arg("outname", help='output name')
    add_arg('--evts-per-record', default=5000, type=int, help='number of events per output file')
    add_arg('--type', default=None, help='which data to process', 
            choices=list(DataSets.__all__))
    add_arg('--type-arg', default="", help='argument to pass to the dataset object constructor if any')
    add_arg("--debug", action='store_true', help='in a debug mode')
    add_arg("--max-evts", type=int, default=-1, help='maximum number of events to process')
    add_arg("--config", help='configuration file for training OR for configuring the dataset', default=None)
    add_arg("--num-workers", help='number of threads', default=1, type=int)
    add_arg("--overwrite", action='store_true',
        help='specifies whether to overwrite existing files with same outname pattern')
    
    ## Graph Configs ##
    add_arg("--connectivity", help='create graph with different types of connectivity, default is fully-connected', default=None, choices=['disconnected', 'KNN'])
    add_arg("--signal", type=int, default=None, choices=[0, 1, 3, 10], help='choose the number of prongs')
    add_arg("--with-edge-features", action='store_true', help='add in edge features')
    add_arg("--with-node-type", action='store_true', help='add in one-hot encodings for node and edge types')
    add_arg("--with-hlv-features", action='store_true', help='add high-level variables to global')
    add_arg("--use-delta-angles", action='store_true', help='use delta eta and phi for the node variables')
    add_arg("--tower-lim", type=int, help='Limits on the number of towers', default=None)
    add_arg("--track-lim", type=int, help='Limits on the number of tracks', default=None)
    add_arg("--cutoff", action='store_true', help='use cutoff of towers and tracks')
    add_arg("--background-dropoff", default=0., type=float, help='the fraction of background not used')
    add_arg("--use-jetPt", help='use JetPt in the nodes', action='store_true')
    add_arg("--use-jetVar", help='use Jet vector variables in global', action='store_true')

    args = parser.parse_args()
    n_evts_per_record = args.evts_per_record
    outname = args.outname

    print("Input Name {}".format(args.input_file))
    print("Output Name {}".format(outname))
    print("{} events per record".format(n_evts_per_record))
    print("Data type:", args.type)
    graph_config = {"connectivity": args.connectivity if args.connectivity is not None else "Fully-Connected",
                    "signal": args.signal,
                    "with_edge_features": args.with_edge_features,
                    "with_node_type": args.with_node_type,
                    "with_hlv_features": args.with_hlv_features,
                    "use_delta_angles": args.use_delta_angles,
                    "tower_lim": args.tower_lim,
                    "track_lim":args.track_lim,
                    "cutoff":args.cutoff,
                    "background_dropoff": args.background_dropoff,
                    "use_jetPt": args.use_jetPt,
                    "use_jetVar": args.use_jetVar}
    
    print("Graph Config:", graph_config)
    print("# of workers:", args.num_workers)

    out_dir = os.path.abspath(os.path.dirname(outname))
    os.makedirs(out_dir, exist_ok=True)
    config_file_backup = open(f"{out_dir}_config.txt", 'w')
    config_file_backup.write(str(graph_config))
    config_file_backup.close()

    if args.type is None:
        print("Specify a data type via --type")
        parser.print_help()
        exit(1)
        
    data = getattr(DataSets, args.type)()
    if args.type == "WTaggerFilteredDataset":
        if args.config is None:
            print("WTaggerFilteredDataset requires model config, --config")
            exit(1)
        else:
            data.set_gnn_config(args.config)

    if args.type in ["FourTopDataset", "WTaggerLeadingJetDataset", "WTaggerFilteredDataset"]:
        if args.signal:
            data.signal()

    if args.type == "RootDataset":
        if args.config is None:
            print("RootDataset requires model yaml config, --config")
            exit(1)
        #call data.set_config_file here to get the yaml file before doing branch reading
        else:
            data.set_config_file(args.config)
        
        data.set_include_particle_type(args.include_particle_type)
    
    if "TauIdentificationDataset" in args.type:
        if args.config is not None:
            data.set_config_file(args.config)

    now = time.time()

    print("GOING TO PROCESS NOW") 
    data.process(filename=args.input_file, outname=outname,\
        n_evts_per_record=n_evts_per_record, debug=args.debug,
        max_evts=args.max_evts, num_workers=args.num_workers, overwrite=args.overwrite, **graph_config)
    
    read_time = time.time() - now
    print("{} finished in {:.1f} mins".format(data.__class__.__name__, read_time/60.))