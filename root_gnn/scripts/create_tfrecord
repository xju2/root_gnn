#!/usr/bin/env python
import time
import os

from root_gnn import datasets as DataSets

if __name__ == "__main__":
    import argparse
    parser = argparse.ArgumentParser(description='Create TFRecord of graphs for training')
    add_arg = parser.add_argument
    add_arg("input_file", help='input file')
    add_arg("outname", help='output name')
    add_arg('--evts-per-record', default=5000, type=int, help='number of events per output file')
    add_arg('--type', default=None, help='which data to process', 
            choices=list(DataSets.__all__))
    add_arg('--type-arg', default="", help='argument to pass to the dataset object constructor if any')
    add_arg("--debug", action='store_true', help='in a debug mode')
    add_arg("--signal", action='store_true', help='the event is signal')
    add_arg("--max-evts", type=int, default=-1, help='maximum number of events to process')
    add_arg("--config", help='configuration file for training OR for configuring the dataset', default=None)
    add_arg("--num-workers", help='number of threads', default=1, type=int)
    add_arg("--overwrite", action='store_true',
        help='specifies whether to overwrite existing files with same outname pattern')
    add_arg("--connectivity", help='create graph with different types of connectivity, default is fully-connected', default=None, choices=['disconnected', 'KNN'])

    args = parser.parse_args()
    n_evts_per_record = args.evts_per_record
    outname = args.outname

    print("Input Name {}".format(args.input_file))
    print("Output Name {}".format(outname))
    print("{} events per record".format(n_evts_per_record))
    print("Data type:", args.type)
    graph_type = args.connectivity if args.connectivity is not None else "Fully-Connected"
    print("Graph type:", graph_type)
    print("# of workers:", args.num_workers)

    out_dir = os.path.abspath(os.path.dirname(outname))
    os.makedirs(out_dir, exist_ok=True)

    if args.type is None:
        print("Specify a data type via --type")
        parser.print_help()
        exit(1)
        
    data = getattr(DataSets, args.type)()
    if args.type == "WTaggerFilteredDataset":
        if args.config is None:
            print("WTaggerFilteredDataset requires model config, --config")
            exit(1)
        else:
            data.set_gnn_config(args.config)

    if args.type in ["FourTopDataset", "WTaggerLeadingJetDataset", "WTaggerFilteredDataset"]:
        if args.signal:
            data.signal()

    if args.type == "RootDataset":
        if args.config is None:
            print("RootDataset requires model yaml config, --config")
            exit(1)
        #call data.set_config_file here to get the yaml file before doing branch reading
        else:
            data.set_config_file(args.config)
        
        data.set_include_particle_type(args.include_particle_type)
    
    if "TauIdentificationDataset" in args.type:
        if args.config is not None:
            data.set_config_file(args.config)

    now = time.time()

    print("GOING TO PROCESS NOW") 
    data.process(filename=args.input_file, outname=outname,\
        n_evts_per_record=n_evts_per_record, debug=args.debug,
        max_evts=args.max_evts, num_workers=args.num_workers, overwrite=args.overwrite, connectivity=args.connectivity)
    
    read_time = time.time() - now
    print("{} finished in {:.1f} mins".format(data.__class__.__name__, read_time/60.))