#!/usr/bin/env python
import numpy as np
import argparse

from random import shuffle
from tensorflow.keras.layers import Input, Dense, Masking, LSTM, concatenate, TimeDistributed, MultiHeadAttention
from tensorflow.keras import Model, losses
from keras.models import load_model
import tensorflow as tf

class LogLoss(losses.BinaryCrossentropy):
    def __init__(self, real_global_weight=5, fake_global_weight=1, **kwargs):
        super(LogLoss, self).__init__(**kwargs)
        self.w_global_real = real_global_weight
        self.w_global_fake = fake_global_weight
    
    def __call__(self, y_true, y_pred, sample_weight=None):
        global_weights = y_true * self.w_global_real \
            + (1 - y_true) * self.w_global_fake
        return super(LogLoss, self).__call__(y_true, y_pred, sample_weight=global_weights)
        
def experimental_model(
        input_shape_1, input_shape_2, input_shape_3,
        dense_units_1_1=32, dense_units_1_2=32,
        lstm_units_1_1=32, lstm_units_1_2=32,
        dense_units_2_1=32, dense_units_2_2=32,
        lstm_units_2_1=32, lstm_units_2_2=32,
        dense_units_3_1=128, dense_units_3_2=128, dense_units_3_3=16,
        merge_dense_units_1=64, merge_dense_units_2=32,
        backwards=False, mask_value=0.0, unroll=True, incl_clusters=True):
    """
    TODO: Documentation
    """
    # Branch 1
    x_1 = Input(shape=input_shape_1)
    mask_1 = Masking(mask_value=mask_value)(x_1)
    shared_dense_1_1 = TimeDistributed(
        Dense(dense_units_1_1, activation="relu"))(mask_1)
    shared_dense_1_2 = TimeDistributed(
        Dense(dense_units_1_2, activation="relu"))(shared_dense_1_1)
    lstm_1_1 = LSTM(lstm_units_1_1, unroll=unroll, go_backwards=backwards,
                    activation="relu", return_sequences=True)(shared_dense_1_2)
    lstm_1_2 = LSTM(lstm_units_1_2, unroll=unroll, go_backwards=backwards,
                    activation="relu")(lstm_1_1)

    # Branch 2
    if incl_clusters:
        x_2 = Input(shape=input_shape_2)
        mask_2 = Masking(mask_value=mask_value)(x_2)
        shared_dense_2_1 = TimeDistributed(
            Dense(dense_units_2_1, activation="relu"))(mask_2)
        shared_dense_2_2 = TimeDistributed(
            Dense(dense_units_2_2, activation="relu"))(shared_dense_2_1)
        lstm_2_1 = LSTM(lstm_units_2_1, unroll=unroll, go_backwards=backwards,
                        activation="relu", return_sequences=True)(shared_dense_2_2)
        lstm_2_2 = LSTM(lstm_units_2_2, unroll=unroll, go_backwards=backwards,
                        activation="relu")(lstm_2_1)

    # Branch 3
    x_3 = Input(shape=input_shape_3)
    dense_3_1 = Dense(dense_units_3_1, activation="relu")(x_3)
    dense_3_2 = Dense(dense_units_3_2, activation="relu")(dense_3_1)
    dense_3_3 = Dense(dense_units_3_3, activation="relu")(dense_3_2)

    # Merge
    if incl_clusters:
        merged_branches = concatenate([lstm_1_2, lstm_2_2, dense_3_3])
    else:
        merged_branches = concatenate([lstm_1_2, dense_3_3])

    merge_dense_1 = Dense(merge_dense_units_1, activation="relu")(
        merged_branches)
    merge_dense_2 = Dense(merge_dense_units_2, activation="relu")(
        merge_dense_1)
    y = Dense(1, activation="sigmoid")(merge_dense_2)

    if incl_clusters:
        return Model(inputs=[x_1, x_2, x_3], outputs=y)
    else:
        return Model(inputs=[x_1, x_3], outputs=y)
    
def rnn_lstm_sum(
        input_shape_1, input_shape_2, input_shape_3,
        dense_units_1_1=32, dense_units_1_2=32,
        lstm_units_1_1=32, lstm_units_1_2=32,
        dense_units_2_1=32, dense_units_2_2=32,
        lstm_units_2_1=32, lstm_units_2_2=32,
        dense_units_3_1=128, dense_units_3_2=128, dense_units_3_3=16,
        merge_dense_units_1=64, merge_dense_units_2=32,
        backwards=False, mask_value=0.0, unroll=True, incl_clusters=True):
    """
    TODO: Documentation
    """
    # Branch 1
    x_1 = Input(shape=input_shape_1)
    mask_1 = Masking(mask_value=mask_value)(x_1)
    shared_dense_1_1 = TimeDistributed(
        Dense(dense_units_1_1, activation="relu"))(mask_1)
    shared_dense_1_2 = TimeDistributed(
        Dense(dense_units_1_2, activation="relu"))(shared_dense_1_1)
    lstm_1_1 = LSTM(lstm_units_1_1, unroll=unroll, go_backwards=backwards,
                    activation="relu", return_sequences=True)(shared_dense_1_2)
    lstm_1_2 = LSTM(lstm_units_1_2, unroll=unroll, go_backwards=backwards,
                    activation="relu")(lstm_1_1)

    # Branch 2
    if incl_clusters:
        x_2 = Input(shape=input_shape_2)
        mask_2 = Masking(mask_value=mask_value)(x_2)
        shared_dense_2_1 = TimeDistributed(
            Dense(dense_units_2_1, activation="relu"))(mask_2)
        shared_dense_2_2 = TimeDistributed(
            Dense(dense_units_2_2, activation="relu"))(shared_dense_2_1)
        lstm_2_1 = LSTM(lstm_units_2_1, unroll=unroll, go_backwards=backwards,
                        activation="relu", return_sequences=True)(shared_dense_2_2)
        lstm_2_2 = LSTM(lstm_units_2_2, unroll=unroll, go_backwards=backwards,
                        activation="relu")(lstm_2_1)

    # Branch 3
    x_3 = Input(shape=input_shape_3)
    dense_3_1 = Dense(dense_units_3_1, activation="relu")(x_3)
    dense_3_2 = Dense(dense_units_3_2, activation="relu")(dense_3_1)
    dense_3_3 = Dense(dense_units_3_3, activation="relu")(dense_3_2)

    # Merge
    if incl_clusters:
        merged_branches = concatenate([lstm_1_2 + lstm_2_2, dense_3_3])
    else:
        merged_branches = concatenate([lstm_1_2, dense_3_3])

    merge_dense_1 = Dense(merge_dense_units_1, activation="relu")(
        merged_branches)
    merge_dense_2 = Dense(merge_dense_units_2, activation="relu")(
        merge_dense_1)
    y = Dense(1, activation="sigmoid")(merge_dense_2)

    if incl_clusters:
        return Model(inputs=[x_1, x_2, x_3], outputs=y)
    else:
        return Model(inputs=[x_1, x_3], outputs=y)
    
def rnn_attn_model(
        input_shape_1, input_shape_2, input_shape_3,
        dense_units_1_1=32, dense_units_1_2=32,
        lstm_units_1_1=32, lstm_units_1_2=32,
        dense_units_2_1=32, dense_units_2_2=32,
        lstm_units_2_1=32, lstm_units_2_2=32,
        dense_units_3_1=128, dense_units_3_2=128, dense_units_3_3=16,
        merge_dense_units_1=64, merge_dense_units_2=32,
        backwards=False, mask_value=0.0, unroll=True, incl_clusters=True):
    """
    TODO: Documentation
    """
    # Branch 1
    x_1 = Input(shape=input_shape_1)
    mask_1 = Masking(mask_value=mask_value)(x_1)
    shared_dense_1_1 = TimeDistributed(
        Dense(dense_units_1_1, activation="relu"))(mask_1)
    shared_dense_1_2 = TimeDistributed(
        Dense(dense_units_1_2, activation="relu"))(shared_dense_1_1)
    lstm_1_1 = MultiHeadAttention(1, lstm_units_1_1)(shared_dense_1_2, shared_dense_1_2)
    lstm_1_2 = MultiHeadAttention(1, lstm_units_1_2)(lstm_1_1, lstm_1_1)
    lstm_1_2 = tf.reduce_sum(lstm_1_2, axis=1)

    # Branch 2
    if incl_clusters:
        x_2 = Input(shape=input_shape_2)
        mask_2 = Masking(mask_value=mask_value)(x_2)
        shared_dense_2_1 = TimeDistributed(
            Dense(dense_units_2_1, activation="relu"))(mask_2)
        shared_dense_2_2 = TimeDistributed(
            Dense(dense_units_2_2, activation="relu"))(shared_dense_2_1)
        lstm_2_1 = MultiHeadAttention(1, lstm_units_2_1)(shared_dense_2_2, shared_dense_2_2)
        lstm_2_2 = MultiHeadAttention(1, lstm_units_2_2)(shared_dense_2_2, shared_dense_2_2)
        #lstm_2_2 = LSTM(lstm_units_2_2, unroll=unroll, go_backwards=backwards,activation="relu")(lstm_2_1)
        lstm_2_2 = tf.reduce_sum(lstm_2_2, axis=1)

    # Branch 3
    x_3 = Input(shape=input_shape_3)
    dense_3_1 = Dense(dense_units_3_1, activation="relu")(x_3)
    dense_3_2 = Dense(dense_units_3_2, activation="relu")(dense_3_1)
    dense_3_3 = Dense(dense_units_3_3, activation="relu")(dense_3_2)

    # Merge
    if incl_clusters:
        merged_branches = concatenate([lstm_1_2, lstm_2_2, dense_3_3])
    else:
        merged_branches = concatenate([lstm_1_2, dense_3_3])

    merge_dense_1 = Dense(merge_dense_units_1, activation="relu")(
        merged_branches)
    merge_dense_2 = Dense(merge_dense_units_2, activation="relu")(
        merge_dense_1)
    y = Dense(1, activation="sigmoid")(merge_dense_2)

    if incl_clusters:
        return Model(inputs=[x_1, x_2, x_3], outputs=y)
    else:
        return Model(inputs=[x_1, x_3], outputs=y)
    
def rnn_sum_model(
        input_shape_1, input_shape_2, input_shape_3,
        dense_units_1_1=32, dense_units_1_2=32,
        lstm_units_1_1=32, lstm_units_1_2=32,
        dense_units_2_1=32, dense_units_2_2=32,
        lstm_units_2_1=32, lstm_units_2_2=32,
        dense_units_3_1=128, dense_units_3_2=128, dense_units_3_3=16,
        merge_dense_units_1=64, merge_dense_units_2=32,
        backwards=False, mask_value=0.0, unroll=True, incl_clusters=True):
    """
    TODO: Documentation
    """
    # Branch 1
    x_1 = Input(shape=input_shape_1)
    mask_1 = Masking(mask_value=mask_value)(x_1)
    shared_dense_1_1 = TimeDistributed(
        Dense(dense_units_1_1, activation="relu"))(mask_1)
    shared_dense_1_2 = TimeDistributed(
        Dense(dense_units_1_2, activation="relu"))(shared_dense_1_1)
    lstm_1_1 = MultiHeadAttention(1, lstm_units_1_1)(shared_dense_1_2, shared_dense_1_2)
    lstm_1_2 = MultiHeadAttention(1, lstm_units_1_2)(lstm_1_1, lstm_1_1)
    lstm_1_2 = tf.reduce_sum(lstm_1_2, axis=1)

    # Branch 2
    if incl_clusters:
        x_2 = Input(shape=input_shape_2)
        mask_2 = Masking(mask_value=mask_value)(x_2)
        shared_dense_2_1 = TimeDistributed(
            Dense(dense_units_2_1, activation="relu"))(mask_2)
        shared_dense_2_2 = TimeDistributed(
            Dense(dense_units_2_2, activation="relu"))(shared_dense_2_1)
        lstm_2_1 = MultiHeadAttention(1, lstm_units_2_1)(shared_dense_2_2, shared_dense_2_2)
        lstm_2_2 = MultiHeadAttention(1, lstm_units_2_2)(shared_dense_2_2, shared_dense_2_2)
        #lstm_2_2 = LSTM(lstm_units_2_2, unroll=unroll, go_backwards=backwards,activation="relu")(lstm_2_1)
        lstm_2_2 = tf.reduce_sum(lstm_2_2, axis=1)

    # Branch 3
    x_3 = Input(shape=input_shape_3)
    dense_3_1 = Dense(dense_units_3_1, activation="relu")(x_3)
    dense_3_2 = Dense(dense_units_3_2, activation="relu")(dense_3_1)
    dense_3_3 = Dense(dense_units_3_3, activation="relu")(dense_3_2)

    lstm_f = lstm_1_2 + lstm_2_2
        
    merged_branches = concatenate([lstm_f, dense_3_3])

    merge_dense_1 = Dense(merge_dense_units_1, activation="relu")(
        merged_branches)
    merge_dense_2 = Dense(merge_dense_units_2, activation="relu")(
        merge_dense_1)
    y = Dense(1, activation="sigmoid")(merge_dense_2)

    if incl_clusters:
        return Model(inputs=[x_1, x_2, x_3], outputs=y)
    else:
        return Model(inputs=[x_1, x_3], outputs=y)
    
    
def rnn_dense_model(
        input_shape_1, input_shape_2, input_shape_3,
        dense_units_1_1=32, dense_units_1_2=32,
        lstm_units_1_1=32, lstm_units_1_2=32,
        dense_units_2_1=32, dense_units_2_2=32,
        lstm_units_2_1=32, lstm_units_2_2=32,
        dense_units_3_1=128, dense_units_3_2=128, dense_units_3_3=16,
        merge_dense_units_1=64, merge_dense_units_2=32,
        backwards=False, mask_value=0.0, unroll=True, incl_clusters=True):
    """
    TODO: Documentation
    """
    # Branch 1
    x_1 = Input(shape=input_shape_1)
    mask_1 = Masking(mask_value=mask_value)(x_1)
    shared_dense_1_1 = TimeDistributed(
        Dense(dense_units_1_1, activation="relu"))(mask_1)
    shared_dense_1_2 = TimeDistributed(
        Dense(dense_units_1_2, activation="relu"))(shared_dense_1_1)
    lstm_1_1 = Dense(lstm_units_1_1)(shared_dense_1_2)
    lstm_1_2 = Dense(lstm_units_1_2)(lstm_1_1)
    lstm_1_2 = tf.reduce_sum(lstm_1_2, axis=1)

    # Branch 2
    if incl_clusters:
        x_2 = Input(shape=input_shape_2)
        mask_2 = Masking(mask_value=mask_value)(x_2)
        shared_dense_2_1 = TimeDistributed(
            Dense(dense_units_2_1, activation="relu"))(mask_2)
        shared_dense_2_2 = TimeDistributed(
            Dense(dense_units_2_2, activation="relu"))(shared_dense_2_1)
        lstm_2_1 = Dense(lstm_units_2_1)(shared_dense_2_2)
        lstm_2_2 = Dense(lstm_units_2_2)(shared_dense_2_2)
        #lstm_2_2 = LSTM(lstm_units_2_2, unroll=unroll, go_backwards=backwards,activation="relu")(lstm_2_1)
        lstm_2_2 = tf.reduce_sum(lstm_2_2, axis=1)

    # Branch 3
    x_3 = Input(shape=input_shape_3)
    dense_3_1 = Dense(dense_units_3_1, activation="relu")(x_3)
    dense_3_2 = Dense(dense_units_3_2, activation="relu")(dense_3_1)
    dense_3_3 = Dense(dense_units_3_3, activation="relu")(dense_3_2)

    lstm_f = lstm_1_2 + lstm_2_2
        
    merged_branches = concatenate([lstm_f, dense_3_3])

    merge_dense_1 = Dense(merge_dense_units_1, activation="relu")(
        merged_branches)
    merge_dense_2 = Dense(merge_dense_units_2, activation="relu")(
        merge_dense_1)
    y = Dense(1, activation="sigmoid")(merge_dense_2)

    if incl_clusters:
        return Model(inputs=[x_1, x_2, x_3], outputs=y)
    else:
        return Model(inputs=[x_1, x_3], outputs=y)


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='Create sequences for RNN')
    parser.add_argument('ditau_path')
    parser.add_argument('qcd_path')
#    parser.add_argument('--prongs',default='1', choices=['1','3'], help='Number of prongs')
    parser.add_argument('--model_path','-o',default=None, help='Model path')
    parser.add_argument('--loss-weights', '-l', default=None, type=int, help='Loss weight')
    parser.add_argument('--name', '-n', default=None, help='model name')
#    parser.add_argument('--prefix','-p',default=None, help='Physics process or prefix')
#    parser.add_argument('--inclusive', action='store_true')
#    parser.add_argument('--pileup', choices=['','_PU','_HL'])
    args = parser.parse_args()

    Higgs_ditau='Ntuple_Higgs_ditau_processed.root'
    Z_ditau='Ntuple_Z_ditau_processed.root'
    ditau_PU='Ntuple_ditau_PU_processed.root'
    ditau='Ntuple_ditau_processed.root'
    qcd='Ntuple_qcd_processed.root'
    ttbar_PU='Ntuple_ttbar_PU_processed.root'
    ttbar='Ntuple_ttbar_processed.root'
    src='/global/cscratch1/sd/lreed'


    input_shape_1 = (10,6)
    input_shape_2 = (6,4)
    input_shape_3 = (8)

    ditau_data = np.load(args.ditau_path)
    qcd_data = np.load(args.qcd_path)
    
    x = [np.concatenate([ditau_data['track_info'],qcd_data['track_info']]),np.concatenate([ditau_data['cluster_info'],qcd_data['cluster_info']]),np.concatenate([ditau_data['hlv_info'],qcd_data['hlv_info']])]
    y = np.concatenate([ditau_data['labels'],qcd_data['labels']])


    if args.model_path is None:
        rnn_model = load_model(args.model_path)
    else:
        if args.name is None:
            rnn_model = experimental_model(input_shape_1,input_shape_2,input_shape_3)
        elif args.name == 'attn':
            rnn_model = rnn_attn_model(input_shape_1,input_shape_2,input_shape_3)
        elif args.name == 'dense':
            rnn_model = rnn_dense_model(input_shape_1,input_shape_2,input_shape_3)
        elif args.name == 'sum':
            rnn_model = rnn_sum_model(input_shape_1,input_shape_2,input_shape_3)
        elif args.name == 'lstm_sum':
            rnn_model = rnn_lstm_sum(input_shape_1,input_shape_2,input_shape_3)
             
    
    filepath = args.model_path
    #if args.inclusive:
        #filepath = args.model_path+f'/rnn_{args.prongs}prong_inc{pileup}'
    #else:
        #filepath = args.model_path+f'/rnn_{args.prongs}prong{pileup}'
    if args.loss_weights is not None:
        loss = LogLoss(args.loss_weights, 1)
    else:
        loss='binary_crossentropy'

    rnn_model.compile(optimizer='adam', loss=loss, metrics=[tf.keras.metrics.AUC()])
    es = tf.keras.callbacks.EarlyStopping(monitor='val_auc', patience=10)
    ckpt = tf.keras.callbacks.ModelCheckpoint(monitor='val_auc', filepath=filepath, save_best_only=True)

    rnn_model.fit(x,y,batch_size=500,epochs=500,shuffle=True,validation_split=0.11,callbacks=[es,ckpt])
    rnn_model.save(filepath)

